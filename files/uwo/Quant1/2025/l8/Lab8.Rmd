---
title: "POLISCI 9590 - Lab8: Association Tests"
author: "William Poirier"
date: "2025-11-17"
output: 
  webexercises::webexercises_default:
      toc: true
      toc_float:
        collapsed: true
      theme: journal
---
<style>
.list-group-item.active, .list-group-item.active:hover, .list-group-item.active:focus {
    z-index: 2;
    color: #ffffff;
    background-color: #4F2683;
    border-color: #4F2683;
}
:root {
    --incorrect: rgb(218, 165, 32);
    --incorrect_alpha: rgba(218, 165, 32, 0.25);
    --correct: rgb(160, 32, 240);
    --correct_alpha: rgba(160, 32, 240, 0.25);
    --highlight: #4F2683;
}

a {
    color: #4F2683;
    text-decoration: none;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,message = FALSE,warning = FALSE)
#install.packages("webexercises")
#install.packages("forcats)
library(webexercises)
library(tidyverse)
library(DAMisc)
```

## 0. What we'll be doing

1. The %in% operator
2. Association Tests


## 1. The %in% operator

```{r}
vec1 <- c(1,2,3,NA,5,6)
vec2 <- c(1,2,3,4,1,2)

# The == operator
vec1 == vec2

# The %in% operator
vec1 %in% vec2

# This has important consequences
library(tidyverse)
library(rio)
cses <- import("cses5.rdata")

# Code in lab 7
tmp1 <- cses |>
  mutate(E2003 = ifelse(E2003>96,NA,E2003),
         educ_low=ifelse(E2003 %in% c(1:3,96),1,0),
         educ_mid=ifelse(E2003 %in% 4:5,1,0),
         educ_hig=ifelse(E2003 %in% 6:9,1,0))

table(cses$E2003,tmp1$educ_low,useNA="ifany")

# What you should be doing in assignment 3
tmp2 <- cses |>
  mutate(educ_low=ifelse(E2003 %in% c(1:3,96),1,ifelse(E2003 >96,NA,0)),
         educ_mid=ifelse(E2003 %in% 4:5,1,      ifelse(E2003 >96,NA,0)),
         educ_hig=ifelse(E2003 %in% 6:9,1,      ifelse(E2003 >96,NA,0)))

table(cses$E2003,tmp2$educ_low,useNA="ifany")

```


## 2. Association Tests

### 2.1 Contigency tables/cross-tabs

The frequency of each combination made by two variables.

- For frequencies: `table()`
- For percentages: `prop.table(table(),margin=2)` 
  - Here `margin=2` implies that the columns will sum up to 100.
  - `margin=1` makes it so that rows sum up to 100.
  - Usually, put the DV in the rows and IV in the columns.
- There is also a `DAMisc` function: `xt`

```{r}
# install.packages("wooldridge")
data(happiness, package="wooldridge")
dat <- happiness %>% 
  slice(sample(1:nrow(happiness),3000)) %>% 
  mutate(happy=fct_rev(droplevels(happy)),
         income=droplevels(income),
         attend=droplevels(attend),
         divorce=droplevels(divorce),
         workstat=droplevels(workstat),
         wrkFTime=ifelse(workstat=="working fulltime",1,0),
         workstat_cat=case_when(workstat=="working fulltime"~"Fulltime",
                                workstat=="working parttime" ~"Parttime",
                                workstat=="retired"~"Retired",
                                workstat=="keeping house"~"Keeping house",
                                workstat %in% c("temp not working",
                                                "unempl, laid off",
                                                "retired",
                                                "school",
                                                "other") ~ "Unemployed",
                                TRUE~ NA),
         educMoreHS=ifelse(educ>=13,1,0),
         kids=ifelse(babies==0&preteen==0&teens==0,0,1))

prop.table(table(dat$happy,dat$reg16),margin = 2)
```

```{r}
tab <- xt(dat,"happy","reg16")
tab$tab
tab$stats
```

You might ask whether those two variables are related in the population or not:

- $H_0:$ Variables X and Y are independent in the population.
- $H_A:$ Variables X and Y are not independent in the population.

You'll learn about the $\chi^2$ test on Wednesday.

### 2.2 Which test?

| Test | Data type | Why do we use it? | Interpretation|
|:-----:|:-----:|:------------|:------------|
|$\chi^2$| Nominal   | Are the frequencies of the contingency table of the two variables randomly distributed?|$H_0$: Frequencies are randomly distributed.|
|$\phi$| Nominal </br> (2x2 table) |How strong is the relationship (-1 to 1)?|0=no </br> >0.1=moderate </br> >0.3=strong|
|Cramer's V|Nominal </br> (>2x2 table)|How strong is the relationship (-1 to 1)?|0=no </br> >0.1=moderate </br> >0.3=strong|
|$\lambda$|Nominal|What is the exact improvement in error in predicting DV by knowing IV?|Percent improvement|
|$\gamma$| Ordinal | How strong is the relationship (-1 to 1)?|0=no </br> >0.1=moderate </br> >0.3=strong|
|Somer's d| Ordinal| How strong is the relationship (-1 to 1)? </br> (A $\gamma$ that accounts for ties on IV)|0=no </br> >0.1=moderate </br> >0.3=strong|
|Kendall's $\tau_b$| Ordinal| How strong is the relationship (-1 to 1)? </br> (A $\gamma$ that accounts for ties on IV and DV)|0=no </br> >0.1=moderate </br> >0.3=strong|
|Spearman's $\rho_s$| Ordinal |How strong is the relationship (-1 to 1)? </br> (Talk of a correlation. Testing for monotonic relationship)| 0=no </br> >0.1=moderate </br> >0.3=strong|
|Pearson r|Continuous|How strong is the relationship (-1 to 1)? </br> (Talk of a correlation. Testing for linear relationship)|0=no </br> >0.1=moderate </br> >0.3=strong|

- \\(°□°)/ ''Greek ABCs!'' \\(°□°)/ 

| Letter |  Name  | Letter |  Name  |
|:------:|:------:|:------:|:------:|
|$A \alpha$             |Alpha  |$N\nu$            |Nu|
|$B \beta$              |Beta   |$\Xi\xi$          |Xi|
|$\Gamma\gamma$         |Gamma  |$O\omicron$       |Omicron|
|$\Delta\delta$         |Delta  |$\Pi\pi$          |Pi|
|$E\epsilon/\varepsilon$|Epsilon|$P\rho$           |Rho|
|$Z\zeta$               |Zeta   |$\Sigma\sigma$    |Sigma|
|$H\eta$                |Eta    |$T\tau$           |Tau|
|$\Theta\theta$         |Theta  |$\Upsilon\upsilon$|Upsilon|
|$I\iota$               |Iota   |$\Phi\phi$        |Phi|
|$K\kappa$              |Kappa  |$X\chi$           |Chi|
|$\Lambda\lambda$       |Lambda |$\Psi\psi$        |Psi|
|$M\mu$                 |Mu     |$\Omega\omega$    |Omega|

- One function to rule them all: `uwo4419::makeStats`

### 2.3 Nominal Association Test Example

Using the variables `income`,`attend`,`wrkFTime`,`educMoreHS`,`kids`, calculate measures of association for the relationship between each of these and `happy`. Make a plot that shows the effects of each variable. 

```{r}
# Stats
library(uwo4419)
stat_income     <- makeStats(dat$happy,dat$income,    lambda=T,cramersV=T,chisq=T,gamma=T,d=T,taub=T,rho=T)
stat_attend     <- makeStats(dat$happy,dat$attend,    lambda=T,cramersV=T,chisq=T,gamma=T,d=T,taub=T,rho=T)
stat_wrkFTime   <- makeStats(dat$happy,dat$wrkFTime,  lambda=T,cramersV=T,chisq=T,gamma=T,d=T,taub=T,rho=T)
stat_educMoreHS <- makeStats(dat$happy,dat$educMoreHS,lambda=T,cramersV=T,chisq=T,gamma=T,d=T,taub=T,rho=T)
stat_kids       <- makeStats(dat$happy,dat$kids,      lambda=T,cramersV=T,chisq=T,gamma=T,d=T,taub=T,rho=T)

# Data for Graph
stats <- bind_rows(stat_income %>% as_tibble(rownames="stat") %>% mutate(var="Income"),
                   stat_attend %>% as_tibble(rownames="stat") %>% mutate(var="Rel. Attend."),
                   stat_wrkFTime %>% as_tibble(rownames="stat") %>% mutate(var="Works"),
                   stat_educMoreHS %>% as_tibble(rownames="stat") %>% mutate(var="Higher Ed."),
                   stat_kids %>% as_tibble(rownames="stat") %>% mutate(var="Kids"))

# Graph
ggplot(stats %>% filter(stat!="Chi-squared"),aes(x=reorder(as.factor(var), statistic, mean), y=statistic)) + 
  geom_bar(stat="identity") + 
  facet_wrap(~stat,scales = "free_x") + 
  theme_minimal() + 
  labs(x="", y="Statistic")+
  coord_flip()
```

### 2.4 Variance, Covariance and Correlation
- Sample variance of Y: $$\sigma_Y=\frac{\sum_{i=1}^n(Y_i-\bar Y)^2}{n-1}$$
- Sample covariance between X and Y: $$cov(X,Y)=\frac{\sum_{i=1}^n (X_i-\bar X)(Y_i-\bar Y)}{n-1}$$
- Pearson correlation between X and Y: $$r_{XY}=\frac{\sum_{i=1}^n(X_i-\bar X)(Y_i-\bar Y)}{\sqrt{\sum_{i=1}^n(X_i-\bar X)^2\sum_{i=1}^n(Y_i-\bar Y)^2}}$$
- R functions:
  - Covariance: `cov`
  - Correlation: `cor`
    - There is also `DAMisc::pwCorrMat()` to produce a correlation matrix, more on this in a bit.

```{r}
# A random variable X
X <- rnorm(1000,0,1)

# Another random variable Y
Ypos <- rnorm(1000,0,1)+X
Ynull <- rnorm(1000,0,1)+rnorm(1000,0,1)
Yneg <- rnorm(1000,0,1)-X

FakeData <- data.frame(X=X,Ypos=Ypos,Ynull=Ynull,Yneg=Yneg)

# Variance covariance matrix
cov(FakeData)

# Correlations
cor(FakeData[,1],FakeData[,2:4])

# Vizualisation 
FakeData %>% 
  pivot_longer(starts_with("Y"),values_to = "Y",names_to = "var",names_pattern = "Y(.*)") %>%
  mutate(var=factor(var,levels=c("pos","null","neg"))) %>% 
  ggplot(aes(x=X,y=Y))+
  geom_point(alpha=0.2,color="#4F2683",shape=16)+
  geom_smooth(method="lm",color="#4F2683")+
  facet_wrap(~var)+
  theme_minimal()

```

### 2.5 Functions to copy and paste
```{r}
custom_smooth <- function(data, mapping, 
                          ..., span=.35, pt.alpha=.25, jitter=TRUE) {
  if(jitter){
    pos <- position_jitter(width=2, height=2)
  }else{
    pos <- position_identity()
  }
  ggplot(data, mapping, ...) + 
    geom_point(shape=1, col="gray", 
               position=pos, alpha=pt.alpha) + 
    geom_smooth(method="loess", span=span,
                family="symmetric",
                se=FALSE, col="red") + 
    geom_smooth(method="lm", col="black", se=FALSE) 
}

check_lin <- function(formula, data, ...){
  requireNamespace("fANCOVA")
  D <- get_all_vars(formula, data)
  D <- na.omit(D)
  if(nrow(D) == 0)stop("No non-missing data.\n")
  forms <- sapply(names(D)[-1], \(x)reformulate(x, response=names(D)[1]))
  y <- c(D[,1])
  X <- as.matrix(D[,-1])
  spans <- sapply(1:ncol(X), \(i)fANCOVA::loess.as(X[,i], y, criterion = "gcv")$pars$span)
  los <- lapply(seq_along(forms), \(i)do.call(loess, list(formula=forms[[i]], data=D, span=spans[i])))
  lms <- lapply(seq_along(forms), \(i)do.call(lm, list(formula=forms[[i]], data=D)))
  n <- nrow(D)
  rss0 <- sapply(lms, \(x)sum(x$residuals^2))
  rss1 <- sapply(los, \(x)sum(x$residuals^2))
  df.linmod <- sapply(lms, \(x)x$rank)
  df.lomod <- sapply(los, \(x)x$trace.hat)
  df.res <- n-df.lomod
  F0 <- ((rss0-rss1)/(df.lomod-df.linmod))/(rss1 / df.res)
  pval <- pf(F0, (df.lomod-df.linmod), df.res, lower.tail=FALSE)
  nl_r2 <- sapply(los, \(m)cor(m$y, m$fitted)^2)
  lin_r2 <- sapply(lms, \(m)cor(m$fitted.values, model.response(model.frame(m)))^2)
  out <- data.frame(vbl = names(D)[-1], lin_r2 = lin_r2, nonlin_r2 = nl_r2)
  out$diff <- out$nonlin_r2 - out$lin_r2
  out$p <- pval
  class(out) <- c("checklin", class(out))
  return(out)
}
```

### 2.6 Correlation Examples
```{r}
#install.packages("GGally")
library(GGally)

# Data
data(countymurders, package="wooldridge")
sub <- countymurders %>% 
  filter(year==1996) %>% 
  select(murdrate,arrestrate,percblack,percmale,rpcpersinc)

# Correlations
DAMisc::pwCorrMat(murdrate~arrestrate+percblack+percmale+rpcpersinc,data=sub)

# Visualize
ggpairs(sub,
        lower = list(continuous = wrap(custom_smooth, 
                                       span=.5, 
                                       pt.alpha=1,
                                       jitter=FALSE))) + 
  theme(legend.position = "bottom")

# Test for non-linearity
check_lin(murdrate~arrestrate+percblack+percmale+rpcpersinc,data=sub)
```

```{r,echo=FALSE}
X1 <- rnorm(1000,0,1)
X2 <- rnorm(1000,0,0.2)+0.3*X1
X3 <- rnorm(1000,0,0.2)-X2^2
X4 <- rnorm(1000,0,0.05)+(1/(1+exp(-X1)))
X5 <- -0.1*X1-rnorm(1000,0,1)
X6 <- X2+X3+X4

FakeData2 <- data.frame(X1,X2,X3,X4,X5,X6) 

opts_linear <- c(answer="Linear","Not")
opts_notl <- c("Linear",answer="Not")

opts_sig <- c(answer="Significant","Not")
opts_notsig <- c("Significant",answer="Not")

opts_strong <- c(answer="Strong","Not")
opts_notstr <- c("Strong",answer="Not")
```

#### 2.6.1 X1 and X2

```{r,echo=FALSE}
ggpairs(FakeData2 %>% select(X1,X2),
        lower = list(continuous = wrap(custom_smooth, 
                                       span=.5, 
                                       pt.alpha=1,
                                       jitter=FALSE))) + 
  theme(legend.position = "bottom")
check_lin(X2~X1,data=FakeData2)
```

:::: {style="display: flex;"}
::: {}
- Linearity?
`r longmcq(opts_linear)`
:::
::: {}
- Significance?
`r longmcq(opts_sig)`
:::
::: {}
- Strong relationship?
`r longmcq(opts_strong)`
:::
::::

#### 2.6.2 X1 and X3
```{r,echo=FALSE}
ggpairs(FakeData2 %>% select(X1,X3),
        lower = list(continuous = wrap(custom_smooth, 
                                       span=.5, 
                                       pt.alpha=1,
                                       jitter=FALSE))) + 
  theme(legend.position = "bottom")

check_lin(X3~X1,data=FakeData2)
```

:::: {style="display: flex;"}
::: {}
- Linearity?
`r longmcq(opts_notl)`
:::
::: {}
- Significance?
`r longmcq(opts_notsig)`
:::
::: {}
- Strong relationship?
`r longmcq(opts_strong)`
:::
::::

#### 2.6.3 X1 and X4
```{r,echo=FALSE}
ggpairs(FakeData2 %>% select(X1,X4),
        lower = list(continuous = wrap(custom_smooth, 
                                       span=.5, 
                                       pt.alpha=1,
                                       jitter=FALSE))) + 
  theme(legend.position = "bottom")

check_lin(X4~X1,data=FakeData2)
```

:::: {style="display: flex;"}
::: {}
- Linearity?
`r longmcq(opts_notl)`
:::
::: {}
- Significance?
`r longmcq(opts_sig)`
:::
::: {}
- Strong relationship?
`r longmcq(opts_strong)`
:::
::::

#### 2.6.4 X1 and X5
```{r,echo=FALSE}
ggpairs(FakeData2 %>% select(X1,X5),
        lower = list(continuous = wrap(custom_smooth, 
                                       span=.5, 
                                       pt.alpha=1,
                                       jitter=FALSE))) + 
  theme(legend.position = "bottom")

check_lin(X5~X1,data=FakeData2)
```

:::: {style="display: flex;"}
::: {}
- Linearity?
`r longmcq(opts_linear)`
:::
::: {}
- Significance?
`r longmcq(opts_sig)`
:::
::: {}
- Strong relationship?
`r longmcq(opts_notstr)`
:::
::::

#### 2.6.5 X1 and X6
```{r,echo=FALSE}
ggpairs(FakeData2 %>% select(X1,X6),
        lower = list(continuous = wrap(custom_smooth, 
                                       span=.5, 
                                       pt.alpha=1,
                                       jitter=FALSE))) + 
  theme(legend.position = "bottom")

check_lin(X6~X1,data=FakeData2)
```

:::: {style="display: flex;"}
::: {}
- Linearity?
`r longmcq(opts_notl)`
:::
::: {}
- Significance?
`r longmcq(opts_sig)`
:::
::: {}
- Strong relationship?
`r longmcq(opts_strong)`
:::
::::
